{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "import lxml.html\n",
    "import requests\n",
    "import cssselect\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to document the text-gathering process for analyzing the federal reserve's use of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{}.htm\"\n",
    "base_url = \"https://www.federalreserve.gov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url.format(1936))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    if \"Minutes\" in link.text:\n",
    "        print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_min(year):\n",
    "    r = requests.get(url.format(year))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    minutes = []\n",
    "    \n",
    "    for link in links:\n",
    "        if \"Minutes\" in link.text:\n",
    "            minutes.append(link)\n",
    "            \n",
    "    minutes_links = []\n",
    "    \n",
    "    for link in minutes:\n",
    "        minutes_links.append(requests.compat.urljoin(base_url, link.get('href')))\n",
    "    \n",
    "    year_text = []\n",
    "    for meeting in minutes_links:\n",
    "        r = requests.get(meeting)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        min_text = soup.body.get_text()\n",
    "        min_text = re.sub(\"\\n\", \"\", min_text)\n",
    "        min_text = re.sub(\"\\r\", \"\", min_text)\n",
    "        year_text.append(min_text)\n",
    "        \n",
    "    return(year_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2005\"\n",
    "r = requests.get(url.format(year))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "minutes = []\n",
    "\n",
    "for link in links:\n",
    "    if \"Minutes\" in link.text:\n",
    "            minutes.append(link)\n",
    "            \n",
    "    minutes_links = []\n",
    "    \n",
    "    for link in minutes:\n",
    "        minutes_links.append(requests.compat.urljoin(base_url, link.get('href')))\n",
    "\n",
    "    for mine in minutes_links:\n",
    "        print(mine[-12:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1994 = get_html_min(1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = []\n",
    "for year in range(1994, 2008):\n",
    "    total_text.append(get_html_min(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattext = [item for thing in total_text for item in thing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\"First\": flattext[0],\n",
    "      \"Second\": flattext[1],\n",
    "      \"Third\": flattext[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    with open(\"texts/\" + i + \".txt\", \"w\") as text_file:\n",
    "        text_file.write(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    with open(\"test\" + str(i) + \".txt\", \"w\") as text_file:\n",
    "        text_file.write(flattext[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = []\n",
    "names = os.listdir()\n",
    "for name in names:\n",
    "    if name.endswith(\".txt\"):\n",
    "        txt_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = {}\n",
    "for i in txt_list:\n",
    "    with open(i, \"r\") as text_file:\n",
    "        name = i[:-4]\n",
    "        test_df[name] = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "for doc in text_1994:\n",
    "    doc_tokens.append(nltk.tokenize.word_tokenize(doc))\n",
    "    \n",
    "print(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.federalreserve.gov/fomc/MINUTES/1994/19940204min.htm\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "soup.body.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs(year):\n",
    "    r = requests.get(url.format(year))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    minutes = []\n",
    "    \n",
    "    for link in links:\n",
    "        if \"Minutes\" in link.text:\n",
    "            minutes.append(link)\n",
    "    minutes_links = []\n",
    "    \n",
    "    for link in minutes:\n",
    "        minutes_links.append(requests.compat.urljoin(base_url, link.get('href')))\n",
    "    \n",
    "    directory = \"C:/Users/mjcor/Documents/GitHub/fed_reserve_text_project/pdfs/\" + str(year) + \"/\"\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    for link in minutes_links:\n",
    "        download_pdf(link, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts(year):\n",
    "    r = requests.get(url.format(year))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    minutes = []\n",
    "    \n",
    "    for link in links:\n",
    "        if \"Transcript \" in link.text:\n",
    "            minutes.append(link)\n",
    "    minutes_links = []\n",
    "    \n",
    "    for link in minutes:\n",
    "        minutes_links.append(requests.compat.urljoin(base_url, link.get('href')))\n",
    "    \n",
    "    directory = \"C:/Users/mjcor/Documents/GitHub/fed_reserve_text_project/pdfs/\" + str(year) + \"/\"\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    for link in minutes_links:\n",
    "        download_pdf(link, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url.format(\"2008\"))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "minutes = []\n",
    "for link in links:\n",
    "    if \"Transcript\" in link.text:\n",
    "        minutes.append(link)\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(2008, 2013):\n",
    "    get_transcripts(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "pdfs = []\n",
    "for link in links:\n",
    "    if \"PDF\" in link.contents:\n",
    "        pdfs.append(link.get(\"href\"))\n",
    "        \n",
    "for pdf in pdfs:\n",
    "    if \"minutes\" in pdf:\n",
    "        yr = pdf[-12:-8]\n",
    "        dl_pdf = requests.compat.urljoin(base_url, pdf)\n",
    "        directory = \"C:/Users/mjcor/Documents/GitHub/fed_reserve_text_project/pdfs/\" + str(yr) + \"/\"\n",
    "        try:\n",
    "            os.stat(directory)\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "        download_pdf(dl_pdf, yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1954, 2013):\n",
    "    get_pdfs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_link[0][-19:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, year):\n",
    "    response = requests.get(pdf_url)\n",
    "    filename = pdf_url[-19:-4]\n",
    "    print(filename)\n",
    "    with open('C:/Users/mjcor/Documents/GitHub/fed_reserve_text_project/pdfs/' + str(year) + \"/\" + filename + \".pdf\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in min_link:\n",
    "    download_pdf(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdf(min_link[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?re.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_link[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_links(year):\n",
    "    r = requests.get(url.format(year))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    statements = []\n",
    "    \n",
    "    for link in links:\n",
    "        if link.text == \"Statement\":\n",
    "            statements.append(link)\n",
    "    state_links = []\n",
    "    \n",
    "    for link in statements:\n",
    "        state_links.append(requests.compat.urljoin(base_url, link.get('href')))\n",
    "    return(state_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_texts(links):\n",
    "    dates = []\n",
    "    texts = []\n",
    "\n",
    "    for link in links:\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        dte = soup.find(\"p\", {\"class\": \"article__time\"}).contents\n",
    "        txt = soup.find(\"div\", {\"class\" : \"col-xs-12 col-sm-8 col-md-8\"}).get_text()\n",
    "        txt = re.sub(\"\\n\", \"\", txt)\n",
    "        dates.append(dte[0])\n",
    "        texts.append(txt)\n",
    "\n",
    "    statements_df = pd.DataFrame({\"Dates\": dates,\n",
    "                                  \"Texts\": texts})\n",
    "    return(statements_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_master = pd.DataFrame(columns = [\"Dates\", \"Texts\"])\n",
    "years = range(2006, 2013)\n",
    "for year in years:\n",
    "    print(year)\n",
    "    temp_df = get_statement_texts(get_statement_links(year))\n",
    "    statements_master = statements_master.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url.format(\"2012\"))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "statements = []\n",
    "for link in links:\n",
    "    if link.text == \"Statement\":\n",
    "        statements.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_links = []\n",
    "for link in statements:\n",
    "    state_links.append(requests.compat.urljoin(base_url, link.get('href')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(state_links[0])\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.from_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "texts = []\n",
    "\n",
    "for link in state_links:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    dte = soup.find(\"p\", {\"class\": \"article__time\"}).contents\n",
    "    txt = soup.find(\"div\", {\"class\" : \"col-xs-12 col-sm-8 col-md-8\"}).get_text()\n",
    "    txt = re.sub(\"\\n\", \"\", txt)\n",
    "    dates.append(dte[0])\n",
    "    texts.append(txt)\n",
    "    \n",
    "statements_df = pd.DataFrame({\"Dates\": dates,\n",
    "                              \"Texts\": texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\", {\"class\": \"article__time\"}).contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", {\"class\" : \"col-xs-12 col-sm-8 col-md-8\"}).contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
